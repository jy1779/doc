# 持久卷

> PersistentVolumes
>
> PersistentVolume（PV，持久卷）：对存储抽象实现，使得存储作为集群中的资源。
> PersistentVolumeClaim（PVC，持久卷申请）：PVC消费PV的资源。
>
> Pod申请PVC作为卷来使用，集群通过PVC查找绑定的PV，并Mount给Pod。
>
> 官网：https://kubernetes.io/docs/concepts/storage/persistent-volumes/

### PersistentVolumes -PV

PV类型：

- ### GCEPersistentDisk
- AWSElasticBlockStore
- AzureFile
- AzureDisk
- FC (Fibre Channel)
- FlexVolume
- Flocker
- NFS
- iSCSI
- RBD (Ceph Block Device)
- CephFS
- Cinder (OpenStack block storage)
- Glusterfs
- VsphereVolume
- Quobyte Volumes
- HostPath
- VMware Photon
- Portworx Volumes
- ScaleIO Volumes
- StorageOS

accessModes：#访问模式

- ReadWriteOnce #写
- ReadOnlyMany #读
- ReadWriteMany #读写

Recycling Policy：#回收策略

- Retain #默认：不使用pv，保留数据
- Recycle #自动回收
- Delete

Phase： #pv状态

- Available #可用
- Bound #已被PVC挂载
- Released #pv不可用，手动释放pv
- Failed  #挂载失败

### nfs测试

1.创建pv

```shell
root@master:~/kubernetes/pv# cat nfs-pv.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity: #容量
    storage: 5Gi
  accessModes: #访问模式
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Recycle #自动回收
  nfs:
    path: /data/logs
    server: 192.168.1.72
root@master:~/kubernetes/pv# kubectl apply -f nfs-pv.yaml 
persistentvolume "nfs-pv" created
#查看Pv
root@master:~/kubernetes/pv# kubectl get pv
NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE
nfs-pv    5Gi        RWX            Recycle          Available                                      5s

```

2.创建pvc

```shell
root@master:~/kubernetes/pv# kubectl apply -f  pvc.yaml
persistentvolumeclaim "pvc001" created
root@master:~/kubernetes/pv# cat pvc.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc001
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
#查看Pvc
root@master:~/kubernetes/pv# kubectl get pv,pvc
NAME        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM            STORAGECLASS   REASON    AGE
pv/nfs-pv   5Gi        RWX            Recycle          Bound     default/pvc001                            29m

NAME         STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc/pvc001   Bound     nfs-pv    5Gi        RWX                           5m

```

3.创建nginx测试

```shell
root@master:~/kubernetes/pv# cat pvc-app.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: nginx
      image: nginx
      volumeMounts:
      - mountPath: "/usr/share/nginx/html"
        name: wwwroot
  volumes:
    - name: wwwroot
      persistentVolumeClaim:
        claimName: pvc001
root@master:~/kubernetes/pv# kubectl apply -f  pvc-app.yaml 
pod "mypod" created
#查看挂载目录
root@master:~# kubectl get pods
NAME                     READY     STATUS    RESTARTS   AGE
mypod                    1/1       Running   0          21m
root@master:~# kubectl exec -it mypod bash
root@mypod:/# ls /usr/share/nginx/html/
1.log

```

### glusterfs测试

1.在glusterfs创建index.html文件，目的是将该目录挂载到k8s中的nginx

```shell
root@node01:/data/gv0# cat /data/gv0/index.html 
hello glusterfs!!
```

2.创建endpoints对象

```shell
root@master:~/kubernetes/pv# cat glusterfs-endpoints.json 
{
  "kind": "Endpoints",
  "apiVersion": "v1",
  "metadata": {
    "name": "glusterfs-cluster"
  },
  "subsets": [
    {
      "addresses": [
        {
          "ip": "192.168.1.72"
        }
      ],
      "ports": [
        {
          "port": 1
        }
      ]
    },
    {
      "addresses": [
        {
          "ip": "192.168.1.73"
        }
      ],
      "ports": [
        {
          "port": 1
        }
      ]
    }
  ]
}
#创建endpoints对象
root@master:~/kubernetes/pv# kubectl create -f glusterfs-endpoints.json 
endpoints "glusterfs-cluster" created
#查看endpoints对象
root@master:~/kubernetes/pv# kubectl get ep
NAME                ENDPOINTS                                               AGE
glusterfs-cluster   192.168.1.72:1,192.168.1.73:1                           37s
kubernetes          192.168.1.72:6443                                       12d
nginx-service       172.20.196.143:80,172.20.196.144:80,172.20.196.146:80   12d
#创建endpoints service
root@master:~/kubernetes/pv# cat glusterfs-service.json 
{
  "kind": "Service",
  "apiVersion": "v1",
  "metadata": {
    "name": "glusterfs-cluster"
  },
  "spec": {
    "ports": [
      {"port": 1}
    ]
  }
}

root@master:~/kubernetes/pv# kubectl create -f  glusterfs-service.json 
service "glusterfs-cluster" created
#创建nginx测试
root@master:~/kubernetes/pv# cat nginx-deployment.yaml 
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        volumeMounts:
        - name: glusterfsvol
          mountPath: /usr/share/nginx/html
        ports:
        - containerPort: 80
      volumes:
      - name: glusterfsvol
        glusterfs:
          endpoints: glusterfs-cluster
          path: gv0
          readOnly: false

---

apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
  type: NodePort
root@master:~/kubernetes/pv# kubectl apply -f nginx-deployment.yaml
#测试
root@master:~/kubernetes/pv# curl 192.168.1.73:29440
hello glusterfs!!

```



3.创建glusterfs-pv.yaml 

```shell
root@master:~/kubernetes/pv# cat glusterfs-pv.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: gluster-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  glusterfs:
    endpoints: "glusterfs-cluster"   #IP或域名
    path: "gv0" #glusterfs路径
    readOnly: false #是否为只读
root@master:~/kubernetes/pv# kubectl create -f glusterfs-pv.yaml 
persistentvolume "gluster-pv" created
root@master:~/kubernetes/pv# kubectl get pv
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM            STORAGECLASS   REASON    AGE
gluster-pv   10Gi       RWX            Retain           Available                                             12s
nfs-pv       5Gi        RWX            Recycle          Bound       default/pvc001                            1d

```

4.创建glusterfs-pvc.yaml

```shell
root@master:~/kubernetes/pv# cat gluster-pvc-app.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: nginx
      image: nginx
      volumeMounts:
      - mountPath: "/usr/share/nginx/html"
        name: wwwroot
  volumes:
    - name: wwwroot
      persistentVolumeClaim:
        claimName: pvc002
root@master:~/kubernetes/pv# kubectl get pvc
NAME      STATUS    VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc001    Bound     nfs-pv       5Gi        RWX                           1d
pvc002    Bound     gluster-pv   10Gi       RWX                           1m

```

5.创建mypod测试

```shell
root@master:~/kubernetes/pv# cat gluster-pvc-app.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: nginx
      image: nginx
      volumeMounts:
      - mountPath: "/usr/share/nginx/html"
        name: wwwroot
  volumes:
    - name: wwwroot
      persistentVolumeClaim:
        claimName: pvc002
```

6.访问测试

```shell
root@master:~/kubernetes/pv# k get pods -o wide
NAME                                READY     STATUS    RESTARTS   AGE       IP               NODE
mypod                               1/1       Running   0          26s       172.20.196.153   192.168.1.73
root@master:~/kubernetes/pv# curl  172.20.196.153
hello glusterfs!!
```

